import gradio as gr
from pdf_preprocess import process_pdf
import json
import torch
import requests
from requests.adapters import HTTPAdapter
from doc_retrive import Sparse_retrive

def call_llm(prompt, url="http://127.0.0.1:6666/chat"):
    headers = {"Content-Type": "application/json"}
    data = json.dumps({"prompt": prompt})
    s = requests.Session()
    s.mount('http://', HTTPAdapter(max_retries=3))
    try:
        res = s.post(url, data=data, headers=headers, timeout=600)
        if res.status_code == 200:
            return res.json()['response']
        else:
            return None
    except requests.exceptions.RequestException as e:
        print(e)
        return None

def rag_process(pdf_file, question, top_k, chunk_size, chunk_overlap):
    try:
        txt_file = process_pdf(pdf_file)
        if not txt_file:
            prompt = question
        else:
            prompt = Sparse_retrive(query=question, top_k=top_k, chunk_size=chunk_size, chunk_overlap=chunk_overlap, contents=txt_file).bm_25()
    except:
        prompt = question
    response = call_llm(prompt)
    result = response
    return result

def _gc():
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        
def clear_inputs():
    _gc()
    return None, "", 5, 500, 100, ""
# print('ğŸ“š')
def main():
    with gr.Blocks() as demo:
        gr.Markdown("""<center><font size=8> ğŸ“˜Chat PDF</center>""")
        gr.Markdown(
            """\
<center><font size=3>æœ¬WebUIåŸºäºQwen-0.5B-Chatå¤§æ¨¡å‹ï¼Œæ‚¨å¯ä»¥ç›´æ¥æé—®æˆ–è€…ä¸Šä¼ ä½ çš„PDFæœ¬åœ°çŸ¥è¯†åº“è¿›è¡Œæé—®ã€‚</center>""")
        gr.Markdown("""\
<center><font size=4>
NDLSLM_0.8B-Chat <a href="https://modelscope.cn/models/Ndlcwx/NDLSLM_0.8B-Chat/summary">ğŸ¤– </a> &nbsp ï½œ 
NDLMoe_1.3B-Chat <a href="https://modelscope.cn/models/Ndlcwx/NDLMoe_1.3B-Chat/summary">ğŸ¤– </a> &nbsp ï½œ 
qwen_1.8B-SFT <a href="https://modelscope.cn/models/Ndlcwx/qwen_1.8B-SFT/summary">ğŸ¤– </a> &nbsp ï½œ 
&nbsp<a href="https://github.com/cwxndl/LLM">Githubåœ°å€</a></center>""")
        with gr.Row():
            with gr.Column(scale=0.001):
                pdf_input = gr.File(label="è¯·åœ¨è¿™é‡Œä¸Šä¼ ä½ çš„PDFæ–‡ä»¶ï¼š", elem_id="pdf_input")
            with gr.Column(scale=1):
                question_input = gr.Textbox(label="è¯·åœ¨è¿™é‡Œè¾“å…¥ä½ çš„é—®é¢˜", elem_id="question_input")
                submit_btn = gr.Button("âœˆï¸å¼€å§‹ç”Ÿæˆ", elem_id="submit_button")
                regenerate_button = gr.Button("ğŸ˜ é‡æ–°ç”Ÿæˆ", elem_id="regenerate_button")
                clear_btn = gr.Button("ğŸ§¹æ¸…é™¤å†…å®¹", elem_id="clear_button")
        
        top_k_slider = gr.Slider(minimum=5, maximum=15, value=5, step=1, label="top_k:åœ¨è¿™é‡Œè®¾ç½®æ‚¨éœ€è¦è¿”å›çš„top_kä¸ªæ–‡æœ¬å—", elem_id="top_k_slider")
        chunk_size_slider = gr.Slider(minimum=100, maximum=1000, value=500, step=100, label="Chunk Size:åœ¨è¿™é‡Œè®¾ç½®æ‚¨éœ€è¦çš„æ¯ä¸ªæ–‡æœ¬å—çš„å¤§å°", elem_id="chunk_size_slider")
        chunk_overlap_slider = gr.Slider(minimum=100, maximum=200, value=100, step=100, label="chunk_overlap:åœ¨è¿™é‡Œè®¾ç½®æ‚¨éœ€è¦çš„æ¯ä¸ªæ–‡æœ¬å—çš„é‡å å¤§å°", elem_id="chunk_overlap_slider")
        answer_output = gr.Textbox(label="å½“å‰é—®é¢˜ç­”æ¡ˆï¼š", elem_id="answer_output")
        
        # regenerate_button = gr.Button("é‡æ–°ç”Ÿæˆ", elem_id="regenerate_button")
        # submit_btn = gr.Button("æäº¤", elem_id="submit_button")
        # clear_btn = gr.Button("æ¸…é™¤å†…å®¹", elem_id="clear_button")

        submit_btn.click(rag_process, inputs=[pdf_input, question_input, top_k_slider, chunk_size_slider, chunk_overlap_slider], outputs=answer_output, show_progress=True)
        regenerate_button.click(rag_process, inputs=[pdf_input, question_input, top_k_slider, chunk_size_slider, chunk_overlap_slider], outputs=answer_output, show_progress=True)
        clear_btn.click(clear_inputs, inputs=[], outputs=[pdf_input, question_input, top_k_slider, chunk_size_slider, chunk_overlap_slider, answer_output], show_progress=True)
        gr.Markdown("""\
<font size=2>æ³¨ï¼šæœ¬æ¼”ç¤ºå—Qwençš„è®¸å¯åè®®é™åˆ¶ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®ï¼Œç”¨æˆ·ä¸åº”ä¼ æ’­åŠä¸åº”å…è®¸ä»–äººä¼ æ’­ä»¥ä¸‹å†…å®¹ï¼Œ\
åŒ…æ‹¬ä½†ä¸é™äºä»‡æ¨è¨€è®ºã€æš´åŠ›ã€è‰²æƒ…ã€æ¬ºè¯ˆç›¸å…³çš„æœ‰å®³ä¿¡æ¯ã€‚""")
    demo.launch(share=True)

if __name__ == '__main__':
    main()
