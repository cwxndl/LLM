import gradio as gr
from file_preprocess import process_pdf
import json
import torch
import requests
from requests.adapters import HTTPAdapter
from doc_retrieve import Sparse_retrive
import hashlib

# å…¨å±€å˜é‡ç”¨äºç¼“å­˜PDFå¤„ç†ç»“æœ
cached_pdf_content = None
cached_pdf_hash = None

def calculate_file_hash(file_path):
    hasher = hashlib.md5()
    with open(file_path, 'rb') as afile:
        buf = afile.read()
        hasher.update(buf)
    return hasher.hexdigest()

def call_llm(prompt, url="http://127.0.0.1:6666/chat"):
    headers = {"Content-Type": "application/json"}
    data = json.dumps({"prompt": prompt})
    s = requests.Session()
    s.mount('http://', HTTPAdapter(max_retries=3))
    try:
        res = s.post(url, data=data, headers=headers, timeout=600)
        if res.status_code == 200:
            return res.json()['response']
        else:
            return None
    except requests.exceptions.RequestException as e:
        print(e)
        return None

def rag_process(pdf_file, question, top_k, chunk_size, chunk_overlap):
    print(pdf_file)
    global cached_pdf_content, cached_pdf_hash
    if pdf_file is not None:
        pdf_hash = calculate_file_hash(pdf_file.name)
        if pdf_hash != cached_pdf_hash:
            # å¦‚æœä¼ å…¥äº†æ–°çš„PDFæ–‡ä»¶ï¼Œé‡æ–°å¤„ç†å¹¶ç¼“å­˜
            cached_pdf_content = process_pdf(pdf_file)
            cached_pdf_hash = pdf_hash
    
    try:
        if not cached_pdf_content:
            prompt = question
        else:
            prompt = Sparse_retrive(query=question, top_k=top_k, chunk_size=chunk_size, chunk_overlap=chunk_overlap, contents=cached_pdf_content).bm_25()
    except Exception as e:
        print(e)
        prompt = question
    
    response = call_llm(prompt)
    result = response
    return result

def _gc():
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        
def clear_inputs():
    global cached_pdf_content, cached_pdf_hash
    _gc()
    cached_pdf_content = None  # æ¸…ç©ºç¼“å­˜
    cached_pdf_hash = None  # æ¸…ç©ºç¼“å­˜
    return None, "", 5, 500, 100, ""

def main():
    with gr.Blocks() as demo:
        gr.Markdown("""<center><font size=8> ğŸ˜„Chat Robot </center>""")
        gr.Markdown(
            """\
<center><font size=3>æœ¬WebUIåŸºäºQwenç³»åˆ—å¤§æ¨¡å‹ï¼Œæ‚¨å¯ä»¥ç›´æ¥æé—®æˆ–è€…ä¸Šä¼ ä½ çš„PDFæœ¬åœ°çŸ¥è¯†åº“è¿›è¡Œæé—®ï¼Œæœ¬äººç›®å‰è®­ç»ƒçš„æ¨¡å‹å¦‚ä¸‹ï¼š</center>""")
        gr.Markdown("""\
<center><font size=4>
NDLSLM_0.8B-Chat <a href="https://modelscope.cn/models/Ndlcwx/NDLSLM_0.8B-Chat/summary">ğŸ¤– </a> &nbsp ï½œ 
NDLSLM_0.8B-beta-Chat <a href="https://modelscope.cn/models/Ndlcwx/NDLSLM_0.8B-beta-Chat/summary">ğŸ¤– </a> &nbsp ï½œ 
NDLMoe_1.3B-Chat <a href="https://modelscope.cn/models/Ndlcwx/NDLMoe_1.3B-Chat/summary">ğŸ¤– </a> &nbsp ï½œ
NDLMoe_1.3B-beta-Chat <a href="https://modelscope.cn/models/Ndlcwx/NDLMoe_1.3B-beta-Chat/summary">ğŸ¤– </a> &nbsp ï½œ 
qwen_1.8B-SFT <a href="https://modelscope.cn/models/Ndlcwx/qwen_1.8B-SFT/summary">ğŸ¤– </a> &nbsp ï½œ 
&nbsp<a href="https://github.com/cwxndl/LLM">Githubåœ°å€</a></center>""")
        with gr.Row():
            with gr.Column(scale=0.001):
                pdf_input = gr.File(label="è¯·åœ¨è¿™é‡Œä¸Šä¼ ä½ çš„PDFæ–‡ä»¶ï¼š", elem_id="pdf_input")
            with gr.Column(scale=1):
                question_input = gr.Textbox(label="è¯·åœ¨è¿™é‡Œè¾“å…¥ä½ çš„é—®é¢˜(æ³¨æ„ï¼šä¸Šä¼ æ–‡ä»¶è¾ƒå¤§çš„PDFæ—¶ä¼šåœ¨ç¬¬ä¸€æ¬¡æé—®æ—¶èŠ±è´¹ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼)", elem_id="question_input")
                answer_output = gr.Textbox(label="å½“å‰é—®é¢˜ç­”æ¡ˆï¼š", elem_id="answer_output")
                submit_btn = gr.Button("âœˆï¸å¼€å§‹ç”Ÿæˆ", elem_id="submit_button")
                regenerate_button = gr.Button("ğŸ˜ é‡æ–°ç”Ÿæˆ", elem_id="regenerate_button")
                clear_btn = gr.Button("ğŸ§¹æ¸…é™¤å†…å®¹", elem_id="clear_button")
        
        top_k_slider = gr.Slider(minimum=5, maximum=15, value=5, step=1, label="top_k:åœ¨è¿™é‡Œè®¾ç½®æ‚¨éœ€è¦è¿”å›çš„top_kä¸ªæ–‡æœ¬å—", elem_id="top_k_slider")
        chunk_size_slider = gr.Slider(minimum=100, maximum=1000, value=500, step=100, label="Chunk Size:åœ¨è¿™é‡Œè®¾ç½®æ‚¨éœ€è¦çš„æ¯ä¸ªæ–‡æœ¬å—çš„å¤§å°", elem_id="chunk_size_slider")
        chunk_overlap_slider = gr.Slider(minimum=100, maximum=200, value=100, step=100, label="chunk_overlap:åœ¨è¿™é‡Œè®¾ç½®æ‚¨éœ€è¦çš„æ¯ä¸ªæ–‡æœ¬å—çš„é‡å å¤§å°", elem_id="chunk_overlap_slider")
        # answer_output = gr.Textbox(label="å½“å‰é—®é¢˜ç­”æ¡ˆï¼š", elem_id="answer_output")
        
        submit_btn.click(rag_process, inputs=[pdf_input, question_input, top_k_slider, chunk_size_slider, chunk_overlap_slider], outputs=answer_output, show_progress=True)
        regenerate_button.click(rag_process, inputs=[pdf_input, question_input, top_k_slider, chunk_size_slider, chunk_overlap_slider], outputs=answer_output, show_progress=True)
        clear_btn.click(clear_inputs, inputs=[], outputs=[pdf_input, question_input, top_k_slider, chunk_size_slider, chunk_overlap_slider, answer_output], show_progress=True)
        gr.Markdown("""\
<font size=2>æ³¨æ„ï¼šæ­¤èŠå¤©æœºå™¨äººæ˜¯åŸºäºå¤§æ¨¡å‹ç”Ÿæˆï¼Œå…¶è¾“å‡ºå†…å®¹å¹¶ä¸ä»£è¡¨æœ¬äººè§‚ç‚¹ï¼ç¦æ­¢è®¨è®ºè‰²æƒ…ã€æš´åŠ›ã€è¯ˆéª—ç­‰å†…å®¹ï¼""")
    demo.launch(share=True)

if __name__ == '__main__':
    main()
