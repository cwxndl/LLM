import gradio as gr
from file_preprocess import process_pdf, process_txt, process_word, process_md
import json
import torch
import requests
from requests.adapters import HTTPAdapter
from doc_retrieve import Sparse_retrive
import hashlib
from http import HTTPStatus
import dashscope
from dashscope import Generation
from PIL import Image
from langchain.prompts import ChatPromptTemplate
# from utils import _parse_text
import os
import numpy as np
import random
############################################################
your_api_key = '' #è¿™é‡Œè¦å¡«å†™ä½ è‡ªå·±çš„apiï¼ˆapiå¼€é€šç½‘å€ï¼šhttps://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdnï¼‰
dashscope.api_key= your_api_key
############################################################
             
def classify_task(prompt,url = 'qwen-turbo'):
    messages = [
        {'role': 'user', 'content': prompt}]
    responses = Generation.call(url,
                                messages=messages,
                                result_format='message',  # è®¾ç½®è¾“å‡ºä¸º'message'æ ¼å¼
                                stream=True, # è®¾ç½®è¾“å‡ºæ–¹å¼ä¸ºæµå¼è¾“å‡º
                                incremental_output=True  # å¢é‡å¼æµå¼è¾“å‡º
                                )
    res = ''
    for response in responses:
        if response.status_code == HTTPStatus.OK:
            res+=response.output.choices[0]['message']['content']
        else:
            res = 'é‡åˆ°é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–è€…apiä½™é¢'
    
    return res

input_messages = [{'role': 'system', 'content': 'You are a helpful assistant.'}]
def call_with_stream(prompt,url):
    global input_messages
    print('å½“å‰ç”¨æˆ·é—®é¢˜ï¼š',prompt)
    # input_messages = [
    #     {'role': 'user', 'content': prompt}]
    input_messages.append({'role': 'user', 'content': prompt})
    responses = Generation.call(url,
                                messages=input_messages,
                                result_format='message',  # è®¾ç½®è¾“å‡ºä¸º'message'æ ¼å¼
                                stream=True, # è®¾ç½®è¾“å‡ºæ–¹å¼ä¸ºæµå¼è¾“å‡º
                                incremental_output=True  # å¢é‡å¼æµå¼è¾“å‡º
                                )
    cur_sys = ''
    cur_res = ''
    for response in responses:
        if response.status_code == HTTPStatus.OK:
            cur_sys =response.output.choices[0]['message']['role']
            cur_res+=response.output.choices[0]['message']['content']
            yield response.output.choices[0]['message']['content']
        else:
            yield 'å½“å‰å¯¹è¯å‡ºé”™ï¼Œè¯·åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æ£€æŸ¥æ‚¨çš„ç½‘ç»œä»¥åŠAPIä½™é¢'
            # å¦‚æœå“åº”å¤±è´¥ï¼Œå°†æœ€åä¸€æ¡user messageä»messagesåˆ—è¡¨é‡Œåˆ é™¤ï¼Œç¡®ä¿user/assistantæ¶ˆæ¯äº¤æ›¿å‡ºç°
            input_messages = input_messages[:-1]
            break
    input_messages.append({'role': cur_sys,
                         'content': cur_res})
    print(input_messages)
def image2text_call(image_name,prompt):
    # å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼šå›¾ç‰‡ç”Ÿæˆæ–‡æœ¬
    image_path_input = 'file://./user_files/image/'+image_name
    # 'file://./files/dog_and_girl.jpeg'
    messages = [
        {
            "role": "user",
            "content": [
                {"image": image_path_input},
                {"text": prompt}
            ]
        }
    ]
    responses = dashscope.MultiModalConversation.call(model='qwen-vl-plus',
                                                     messages=messages,
                                                     stream=True,
                                                     incremental_output=True )
    for response in responses:
        if response.status_code == HTTPStatus.OK and response.output.choices[0]['message']['content']:
            yield response.output.choices[0]['message']['content'][0]['text']
        
# å…¨å±€å˜é‡ç”¨äºç¼“å­˜æ–‡æœ¬æ–‡ä»¶å¤„ç†ç»“æœ
cached_file_content = None
cached_file_hash = None
task_res = None
def calculate_file_hash(file_path):
    hasher = hashlib.md5()
    with open(file_path, 'rb') as afile:
        buf = afile.read()
        hasher.update(buf)
    return hasher.hexdigest()

def call_llm(prompt, url): ####ä½ è‡ªå·±éƒ¨ç½²çš„Qwenæ¨¡å‹APIåœ°å€-----é€‚ç”¨äºQwen1.0
    headers = {"Content-Type": "application/json"}
    data = json.dumps({"prompt": prompt})
    s = requests.Session()
    s.mount('http://', HTTPAdapter(max_retries=3))
    try:
        res = s.post(url, data=data, headers=headers, timeout=600)
        if res.status_code == 200:
            return res.json()['response']
        else:
            return None
    except requests.exceptions.RequestException as e:
        print(e)
        return None

def save_text(content,file_name):
    save_dir = "./user_files/text"
    os.makedirs(save_dir, exist_ok=True)
    # è·å–ä¸Šä¼ æ–‡ä»¶çš„æ–‡ä»¶å
    file_name = os.path.basename(file_name.name)
    file_name = file_name.split('.')[0]+'.txt'
    # å®šä¹‰ä¿å­˜è·¯å¾„
    save_path = os.path.join(save_dir, file_name)
    # æ‰“å¼€æ–‡ä»¶å¹¶å†™å…¥å­—ç¬¦ä¸²
    with open(save_path, "w", encoding="utf-8") as file:
        file.write(content)
    print(f"å­—ç¬¦ä¸²å·²å†™å…¥åˆ°: {save_path}")

def agent(file, question, top_k, chunk_size, chunk_overlap, llm_api):
    print(file)
    global cached_file_content, cached_file_hash
    if file is not None:
        file_name = os.path.basename(file.name)
        file_hash = calculate_file_hash(file.name)
        file_end_str = file_name.split('.')[-1]
        print(file_end_str)
        if file_hash != cached_file_hash:
            # å¦‚æœä¼ å…¥äº†æ–°çš„PDFæ–‡ä»¶ï¼Œé‡æ–°å¤„ç†å¹¶ç¼“å­˜
            if 'txt' in file.name:
                cached_file_content = process_txt(file)
                cached_file_hash = file_hash
            elif 'pdf' in file.name: 
                cached_file_content = process_pdf(file)
                cached_file_hash = file_hash
            elif 'doc' in file.name or 'docx' in file.name:
                cached_file_content = process_word(file)
                cached_file_hash = file_hash
            elif 'md' in file.name:
                cached_file_content = process_md(file)
                cached_file_hash = file_hash
            elif 'jpg' in file.name or 'jpeg' in file.name or 'png' in file.name:
                # å›¾ç‰‡å¤„ç†åŠŸèƒ½
                # é¦–å…ˆå°†ç”¨æˆ·è¾“å…¥çš„å›¾ç‰‡ä¿å­˜è‡³æœ¬åœ°æ–‡ä»¶å¤¹ user_files/imageæ–‡ä»¶å¤¹ä¸‹
                save_dir = "./user_files/image"
                os.makedirs(save_dir, exist_ok=True)
                image = Image.open(file)
                # è·å–ä¸Šä¼ æ–‡ä»¶çš„æ–‡ä»¶å
                # file_name = os.path.basename(file.name)
                # å®šä¹‰ä¿å­˜è·¯å¾„
                save_path = os.path.join(save_dir, file_name)
                # ä¿å­˜å›¾ç‰‡
                image.save(save_path)
                print('å½“å‰ç”¨æˆ·å›¾ç‰‡å·²ä¿å­˜è‡³ï¼š',save_path)
        if file_end_str in ['jpg','png','jpeg']: #å¤šæ¨¡æ€ä»»åŠ¡
            image_result = ''
            for text in image2text_call(image_name=file_name,prompt=question):
                image_result+=text
                yield image_result
        else:
            save_text(cached_file_content,file)
            prompt = Sparse_retrive(query=question, top_k=top_k, chunk_size=chunk_size, chunk_overlap=chunk_overlap, contents=cached_file_content).bm_25()
            if llm_api =='qwen-max' :
                result = ""
                for text in call_with_stream(prompt,llm_api):
                    result += text 
                    yield result
            elif llm_api =='qwen-turbo':
                result = ""
                for text in call_with_stream(prompt,llm_api):
                    result += text 
                    yield result
            elif llm_api =="qwen-plus":
                result = ""
                for text in call_with_stream(prompt,llm_api):
                    result += text 
                    yield result
    else:
        global task_res
        angent_task_prompt = ChatPromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«ä»»åŠ¡å¹¶å…·æœ‰é‡å†™promptèƒ½åŠ›çš„æ™ºèƒ½åŠ©æ‰‹ã€‚"
        "å·²çŸ¥ç›®å‰ç³»ç»Ÿæ‹¥æœ‰ä¸¤ç§ä»»åŠ¡ï¼Œç¬¬ä¸€ç§ä»»åŠ¡æ˜¯æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œå³ç”¨æˆ·è¾“å…¥ç›¸åº”çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡é—®é¢˜\n"
        "ç¬¬äºŒç§ä»»åŠ¡æ˜¯å›¾ç‰‡ç”Ÿæˆä»»åŠ¡ï¼Œç”¨æˆ·éœ€è¦æè¿°å…³äºç”Ÿæˆæ»¡è¶³æŸç§è¦æ±‚çš„ç›¸å…³å›¾ç‰‡\n"
        "ç¬¬ä¸‰ç§ä»»åŠ¡æ˜¯å›¾ç‰‡æè¿°ä»»åŠ¡ï¼Œ"
        "ç°åœ¨ä½ éœ€è¦æ ¹æ®ç”¨æˆ·é—®é¢˜æ¥å‡†ç¡®è¯†åˆ«å‡ºè¿™ä¸¤ç§ä»»åŠ¡ï¼Œå¹¶æŒ‰ç…§æˆ‘çš„è¦æ±‚è¿”å›ç›¸åº”çš„ç»“æœï¼šå¦‚æœæ˜¯æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œä½ éœ€è¦è¾“å‡ºï¼šæ–‡æœ¬ç”Ÿæˆï¼›å¦‚æœæ˜¯å›¾ç‰‡ç”Ÿæˆä»»åŠ¡ï¼Œä½ éœ€è¦è¾“å‡ºï¼šå›¾ç‰‡ç”Ÿæˆ\n"
        "è¯·ä½ ç‰¢è®°ä¸Šé¢çš„è§„åˆ™ï¼Œå¯¹ç”¨æˆ·é—®é¢˜ä»»åŠ¡è¿›è¡Œåˆ¤æ–­ï¼Œä½ åªéœ€è¦è¾“å‡ºåˆ¤æ–­ç»“æœå³å¯ï¼Œä¸éœ€è¦è¾“å‡ºåˆ¤æ–­ç†ç”±ï¼Œç”¨æˆ·é—®é¢˜ä¸ºï¼š{q}").format_messages(q=question)
        task_res = classify_task(angent_task_prompt[0].content,"qwen-turbo")
        print('å½“å‰ä»»åŠ¡ï¼š',task_res)
        if 'æ–‡æœ¬ç”Ÿæˆ' in task_res:
            if llm_api =='qwen-max' :
                result = ""
                for text in call_with_stream(question,llm_api):
                    result += text 
                    yield result
            elif llm_api =='qwen-turbo':
                result = ""
                for text in call_with_stream(question,llm_api):
                    result += text 
                    yield result
            elif llm_api =="qwen-plus":
                result = ""
                for text in call_with_stream(question,llm_api):
                    result += text 
                    yield result
        else:
            pass #è¿˜æœªå¼€å‘

def multimode_agent(file, question):
    print(file,type(file))
    # image_array = np.array(file)
    # å°†æ•°ç»„è½¬æ¢ä¸ºPIL Imageå¯¹è±¡
    image = Image.fromarray(file.astype('uint8'), 'RGB')
    # è‹¥è¦ä¿å­˜å›¾åƒï¼Œå¯ä»¥ä½¿ç”¨å¦‚ä¸‹ä»£ç 
    save_dir = "./user_files/image"
    os.makedirs(save_dir, exist_ok=True)
    file_name = 'ç”¨æˆ·å›¾ç‰‡_'+str(random.randint(a=0,b=10000))+'.jpg'
    save_path = os.path.join(save_dir, file_name)
    print(save_path)
    image.save(save_path)
    print('å½“å‰ç”¨æˆ·å›¾ç‰‡å·²ä¿å­˜è‡³ï¼š',save_path)
    image_result = ''
    for text in image2text_call(image_name=file_name,prompt=question):
        image_result+=text
        yield image_result
    # pass 
    
def _gc():
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        
def clear_inputs():
    global cached_file_content, cached_file_hash
    _gc()
    cached_file_content = None  # æ¸…ç©ºç¼“å­˜
    cached_file_hash = None  # æ¸…ç©ºç¼“å­˜
    return None, "", 5, 500, 100, "", "qwen-turbo", None

def clear_history_():
    global input_messages
    input_messages = [{'role': 'system', 'content': 'You are a helpful assistant.'}]
    return "",""

def clear_iamge_inputs():
    return None,"",""
def main():
    with gr.Blocks(theme=gr.themes.Default(primary_hue=gr.themes.colors.red, secondary_hue=gr.themes.colors.pink)) as demo:
        gr.Markdown("""<center><font size=8> ğŸ˜„Chat Robot 2.0 </center>""")
        gr.Markdown(
            """\
<center><font size=3>æœ¬WebUIåŸºäºQwenç³»åˆ—å¤§æ¨¡å‹ï¼Œæ‚¨å¯ä»¥ç›´æ¥æé—®æˆ–è€…ä¸Šä¼ ä½ çš„æœ¬åœ°æ–‡ä»¶çŸ¥è¯†åº“è¿›è¡Œæé—®ï¼Œ2.0ç‰ˆæœ¬æ”¯æŒå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œæœ¬äººç›®å‰è®­ç»ƒçš„æ¨¡å‹å¦‚ä¸‹ï¼š</center>""")
        gr.Markdown("""\
<center><font size=4>
NDLSLM_0.8B-Chat <a href="https://modelscope.cn/models/Ndlcwx/NDLSLM_0.8B-Chat/summary">ğŸ¤– </a> &nbsp ï½œ 
NDLSLM_0.8B-beta-Chat <a href="https://modelscope.cn/models/Ndlcwx/NDLSLM_0.8B-beta-Chat/summary">ğŸ¤– </a> &nbsp ï½œ 
NDLMoe_1.3B-Chat <a href="https://modelscope.cn/models/Ndlcwx/NDLMoe_1.3B-Chat/summary">ğŸ¤– </a> &nbsp ï½œ
NDLMoe_1.3B-beta-Chat <a href="https://modelscope.cn/models/Ndlcwx/NDLMoe_1.3B-beta-Chat/summary">ğŸ¤– </a> &nbsp ï½œ 
qwen_1.8B-SFT <a href="https://modelscope.cn/models/Ndlcwx/qwen_1.8B-SFT/summary">ğŸ¤– </a> &nbsp ï½œ 
&nbsp<a href="https://github.com/cwxndl/LLM">Githubåœ°å€</a></center>""")
        with gr.Tabs():
            with gr.TabItem("æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆä¸Šä¼ æœ¬åœ°æ–‡æœ¬æ–‡ä»¶æˆ–ç›´æ¥æé—®ï¼‰ï¼š"):
                with gr.Row():
                    with gr.Column(scale=0.0001):
                        pdf_input = gr.File(label="è¯·åœ¨è¿™é‡Œä¸Šä¼ ä½ çš„æœ¬åœ°æ–‡ä»¶ï¼ˆå½“å‰æ”¯æŒPDF,txt,word,markdownæ–‡æœ¬ï¼‰", elem_id="pdf_input")
                #         llm_api_dropdown = gr.Dropdown(choices=[
                #     "qwen-max",
                #     "qwen-turbo",
                #     "qwen-plus"
                # ], value="qwen-turbo", label="è¯·åœ¨è¿™é‡Œé€‰æ‹©æ‚¨éœ€è¦çš„å¤§è¯­è¨€æ¨¡å‹ï¼š", elem_id="llm_api_dropdown")
                    with gr.Column(scale=1):
                        question_input = gr.Textbox(label="è¯·åœ¨è¿™é‡Œè¾“å…¥ä½ çš„é—®é¢˜(æ³¨æ„ï¼šä¸Šä¼ æ–‡ä»¶è¾ƒå¤§çš„æ–‡ä»¶æ—¶ä¼šåœ¨ç¬¬ä¸€æ¬¡æé—®æ—¶èŠ±è´¹ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼)", elem_id="question_input")
                        text_answer_output = gr.Textbox(label="å½“å‰é—®é¢˜ç­”æ¡ˆï¼š", elem_id="answer_output")
                        with gr.Row():
                            submit_btn_text = gr.Button("âœˆï¸æäº¤", elem_id="submit_button")
                            regenerate_button = gr.Button("ğŸ˜ é‡æ–°ç”Ÿæˆ", elem_id="regenerate_button")
                            clear_btn = gr.Button("ğŸ§¹æ¸…é™¤å½“å‰ç•Œé¢å†…å®¹", elem_id="clear_button")
                            clear_history = gr.Button("ğŸ§¹æ¸…é™¤å†å²å¯¹è¯å†…å®¹", elem_id="clear_history_button")
                        llm_api_dropdown = gr.Dropdown(choices=[
                    "qwen-max",
                    "qwen-turbo",
                    "qwen-plus"
                ], value="qwen-turbo", label="è¯·åœ¨è¿™é‡Œé€‰æ‹©æ‚¨éœ€è¦çš„å¤§è¯­è¨€æ¨¡å‹ï¼š", elem_id="llm_api_dropdown")
                top_k_slider = gr.Slider(minimum=5, maximum=15, value=5, step=1, label="top_k:åœ¨è¿™é‡Œè®¾ç½®æ‚¨éœ€è¦è¿”å›çš„top_kä¸ªæ–‡æœ¬å—", elem_id="top_k_slider")
                chunk_size_slider = gr.Slider(minimum=100, maximum=1000, value=500, step=100, label="Chunk Size:åœ¨è¿™é‡Œè®¾ç½®æ‚¨éœ€è¦çš„æ¯ä¸ªæ–‡æœ¬å—çš„å¤§å°", elem_id="chunk_size_slider")
                chunk_overlap_slider = gr.Slider(minimum=100, maximum=200, value=100, step=100, label="chunk_overlap:åœ¨è¿™é‡Œè®¾ç½®æ‚¨éœ€è¦çš„æ¯ä¸ªæ–‡æœ¬å—çš„é‡å å¤§å°", elem_id="chunk_overlap_slider")

            
            with gr.TabItem('å¤šæ¨¡æ€ä»»åŠ¡ï¼ˆå›¾ç‰‡ç†è§£ä»»åŠ¡ï¼‰'):
                with gr.Row():
                    image_input = gr.Image()
                    with gr.Column():
                        image_text_input = gr.Textbox(label='è¯·åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜ï¼š')
                        image_text_out = gr.Textbox(label="å½“å‰å¤šæ¨¡æ€é—®é¢˜ç­”æ¡ˆï¼š", elem_id="image_text_out")
                        submit_btn_image = gr.Button("âœˆï¸æäº¤", elem_id="submit_button")
                        regenerate_image_button = gr.Button("ğŸ˜ é‡æ–°ç”Ÿæˆ", elem_id="regenerate_button")
                        clear_image_btn = gr.Button("ğŸ§¹æ¸…é™¤å†…å®¹", elem_id="clear_button")
                    
                
        submit_btn_text.click(agent, inputs=[pdf_input, question_input, top_k_slider, chunk_size_slider, chunk_overlap_slider, llm_api_dropdown], outputs=[text_answer_output], show_progress=True)
        submit_btn_image.click(multimode_agent,inputs=[image_input,image_text_input],outputs=[image_text_out], show_progress=True)
 
        regenerate_button.click(agent, inputs=[pdf_input, question_input, top_k_slider, chunk_size_slider, chunk_overlap_slider, llm_api_dropdown], outputs=[text_answer_output], show_progress=True)
        regenerate_image_button.click(multimode_agent,inputs=[image_input,image_text_input],outputs=[image_text_out], show_progress=True)
        
        clear_btn.click(clear_inputs, inputs=[], outputs=[pdf_input, question_input, top_k_slider, chunk_size_slider, chunk_overlap_slider, text_answer_output, llm_api_dropdown], show_progress=True)
        clear_history.click(clear_history_,inputs=[],outputs=[question_input,text_answer_output],show_progress=True)
        clear_image_btn.click(clear_iamge_inputs,inputs=[],outputs=[image_input,image_text_input,image_text_out])
        gr.Markdown("""\
<font size=3>æ³¨æ„ï¼šæ­¤èŠå¤©æœºå™¨äººæ˜¯åŸºäºå¤§æ¨¡å‹ç”Ÿæˆï¼Œå…¶è¾“å‡ºå†…å®¹å¹¶ä¸ä»£è¡¨æœ¬äººè§‚ç‚¹ï¼ç¦æ­¢è®¨è®ºè‰²æƒ…ã€æš´åŠ›ã€è¯ˆéª—ç­‰å†…å®¹ï¼""")
    demo.queue().launch(share=True)

if __name__ == '__main__':
    main()
