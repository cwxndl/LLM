# SLMs&MOE  
created by NDL
<p align="left">
    <a href="README_CN.md">ä¸­æ–‡</a>&nbsp ï½œ &nbspEnglish&nbsp ï½œ &nbsp<a href="README_JA.md">æ—¥æœ¬èª</a> ï½œ &nbsp<a href="README_FR.md">FranÃ§ais</a> ï½œ &nbsp<a href="README_ES.md">EspaÃ±ol</a>
</p>
<br><br>
<p align="center">
        ğŸ¤— <a href="https://huggingface.co/Ndlcwx">Hugging Faceæ¨¡å‹ä¸»é¡µ</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/profile/Ndlcwx">ModelScopeæ¨¡å‹ä¸»é¡µ</a>&nbsp&nbsp 
<br>
        
> [!IMPORTANT]
> - 2024.5.22: åŠ å…¥åŸºäºGradioä¸FastAPIæ­å»ºçš„èŠå¤©æœºå™¨äººåŠŸèƒ½ï¼Œæ”¯æŒä¸Šä¼ PDFå¤–æŒ‚ä½ çš„æœ¬åœ°çŸ¥è¯†åº“ï¼ˆRAGï¼‰ï¼Œ[æŸ¥çœ‹è¯¦æƒ…](https://github.com/cwxndl/Chat-bot/tree/main)  
> - å®‰è£…ä¾èµ–å‘½ä»¤ï¼šstep1:cd chat-bot ,step2: pip install -r chat-bot-requirement.txt
> - 2024.5.28ï¼šæ›´æ–°chatbot 2.0:åŸºäºé€šä¹‰åƒé—®APIä»¥åŠLangchain å…¨æ–°æ„å»ºæ–°çš„webuiæ¡†æ¶ï¼Œæ”¯æŒ**å¤šæ¨¡æ€å¯¹è¯**ï¼Œ**å¤šè½®å¯¹è¯**ï¼Œ**å¤šç±»å‹æ–‡ä»¶è¾“å…¥(PDF,markdown,txt,docx)ä»¥åŠå›¾ç‰‡è¾“å…¥**,ç›¸åº”æ›´å¿«ï¼Œå›ç­”æ›´å‡†ç¡®

- èŠå¤©æœºå™¨äºº1.0ç¤ºä¾‹ï¼ˆæµ‹è¯•RAGä¸apiç«¯å£ï¼‰
<div align="center">
<img src="./assets/chat_bot1.gif" width="750" >
</div>

- èŠå¤©æœºå™¨äºº2.0å¤šè½®å¯¹è¯èƒ½åŠ›ç¤ºä¾‹ï¼š
<div align="center">
<img src="./assets/multi1.gif" width="750" >
</div>

- èŠå¤©æœºå™¨äºº2.0 PDFå¤„ç†èƒ½åŠ›ç¤ºä¾‹ï¼š
<div align="center">
<img src="./assets/PDFå¤„ç†.gif" width="750" >
</div>

- èŠå¤©æœºå™¨äºº2.0 markdownå¤„ç†èƒ½åŠ›ç¤ºä¾‹ï¼š
<div align="center">
<img src="./assets/markdown.gif" width="750" >
</div>

- èŠå¤©æœºå™¨äºº2.0 wordå¤„ç†èƒ½åŠ›ç¤ºä¾‹ï¼š
<div align="center">
<img src="./assets/wordå¤„ç†.gif" width="750" >
</div>

- èŠå¤©æœºå™¨äºº2.0 txtå¤„ç†èƒ½åŠ›ç¤ºä¾‹ï¼š
<div align="center">
<img src="./assets/txtå¤„ç†.gif" width="750" >
</div>

- èŠå¤©æœºå™¨äºº2.0 å¤šæ¨¡æ€å¯¹è¯å¤„ç†èƒ½åŠ›ç¤ºä¾‹ï¼š
<div align="center">
<img src="./assets/multimode.gif" width="750" >
</div>

ğŸ‘‰ ä»¥å¾€é¡¹ç›®åœ°å€ï¼š
- [æœºå™¨å­¦ä¹ ç®—æ³•å¤ç°--æ­£åœ¨å®Œå–„](https://github.com/cwxndl/Machine-Learning-By-Python-R-Matlab)
        
- [2023-ç¾èµ›æ˜¥å­£èµ›è§£ç­”](https://github.com/cwxndl/2023-MCM-Y)
        
ğŸ˜„æœ¬é¡¹ç›®æ”¯æŒä»¥ä¸‹ä»»åŠ¡ï¼š
- æ”¯æŒé¢„è®­ç»ƒ
- æ”¯æŒå¢é‡é¢„è®­ç»ƒ
- æ”¯æŒç›‘ç£å¾®è°ƒ-SFT
- æ”¯æŒLoraå¾®è°ƒ

ğŸ¡å½“å‰å·²è®­ç»ƒå¥½çš„æ¨¡å‹ï¼š
- [NDLSLM_0.8B-base](https://modelscope.cn/models/Ndlcwx/NDLSLM_0.8B-base/summary)
- [NDLSLM_0.8B-beta-base](https://modelscope.cn/models/Ndlcwx/NDLSLM_0.8B-beta-base/summary)
- [NDLSLM_0.8B-Chat](https://modelscope.cn/models/Ndlcwx/NDLSLM_0.8B-Chat/summary)
- [NDLSLM_0.8B-Lora-Chat]():å¾…ä¸Šä¼ 
- [NDLSLM_0.8B-beta-Chat](https://modelscope.cn/models/Ndlcwx/NDLSLM_0.8B-beta-Chat/summary)
- [NDLMoe_1.3B-base](https://modelscope.cn/models/Ndlcwx/NDLMoe_1.3B-base/summary)
- [NDLMoe_1.3B-Chat](https://modelscope.cn/models/Ndlcwx/NDLMoe_1.3B-Chat/summary)
- [NDLMoe_1.3B-beta-Chat](https://modelscope.cn/models/Ndlcwx/NDLMoe_1.3B-beta-Chat/summary)
- [åŸºäºNDLSLM_0.8B-Chatçš„Lora-å‘½åå®ä½“è¯†åˆ«]():å¾…ä¸Šä¼ 
- [ä½¿ç”¨æœ¬é¡¹ç›®çš„SFTæ¡†æ¶å¯¹Qwen_1.8B-baseè¿›è¡ŒSFTè®­ç»ƒçš„æ¨¡å‹](https://modelscope.cn/models/Ndlcwx/qwen_1.8B-SFT/summary):2024-5-10æ›´æ–°

âœï¸å¾…åšä»»åŠ¡ï¼š
- DPOä»»åŠ¡
- éƒ¨ç½²æ¡†æ¶
- vllmåŠ é€Ÿæ¡†æ¶

**ğŸ˜Šæ¨èä½¿ç”¨Modelscopeä¸‹è½½æœ¬é¡¹ç›®çš„æ¨¡å‹**ï¼š
```python
# ä¾‹å¦‚ä¸‹è½½æœ¬é¡¹ç›®çš„NDLMoe_1.3B-Chatæ¨¡å‹åˆ°ä½ çš„æœ¬åœ°æ–‡ä»¶å¤¹
path = <your_path_name>
#æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('Ndlcwx/NDLMoe_1.3B-Chat',cache_dir = path)
```


## ç¯å¢ƒä¾èµ–
```bash
# å¦‚æœæƒ³å¿«é€Ÿå¼€å§‹é¢„è®­ç»ƒï¼Œè¿™é‡Œè¯·å…ˆå®‰è£…éœ€è¦çš„pipåŒ…
pip install -r requirements.txt
```

## ğŸ˜‰ é¢„è®­ç»ƒç»†èŠ‚
<font color=Red>å¦‚æœæƒ³ç›´æ¥å¼€å¯æ‚¨çš„é¢„è®­ç»ƒï¼Œè¯·ç›´æ¥é˜…è¯» **æ•°æ®é¢„å¤„ç†**ä»¥åŠåé¢çš„å†…å®¹ã€‚</font>

### 1.ä»é›¶è®­ç»ƒè‡ªå·±çš„åˆ†è¯æ¨¡å‹
å‚è€ƒé¡¹ç›®ä¸ºï¼šhttps://github.com/charent/Phi2-mini-Chinese

NLPä»»åŠ¡ä¸ä¸€èˆ¬çš„æœºå™¨å­¦ä¹ ä»»åŠ¡æœ‰æ‰€ä¸åŒï¼Œéœ€è¦å¤„ç†çš„æ˜¯æ–‡æœ¬æ•°æ®ã€‚åˆ©ç”¨åˆ†è¯å™¨å¯ä»¥å°†æ–‡æœ¬è¿›è¡Œåˆç†çš„åˆ†è¯ï¼Œè¿™æ ·å¯¹äºè¾“å…¥çš„æ¯ä¸€æ®µæ–‡æœ¬ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥å°†å…¶æ˜ å°„åˆ°æ•°å€¼ç©ºé—´å†…ï¼ˆtoken_idï¼‰ï¼Œè€Œæ¯ä¸€ä¸ªtoken_idéƒ½å¯ä»¥é€šè¿‡embeddingè½¬æ¢ä¸ºç¨ å¯†çš„å‘é‡ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œé€šè¿‡transformeræ¶æ„ä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†tokenåºåˆ—ä¸­æ‰€æ¶µç›–çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå› æ­¤æœ‰å¿…è¦äº†è§£æ¸…æ¥šå¤§æ¨¡å‹ä¸­çš„åˆ†è¯æ–¹æ³•ã€‚

æ ¹æ®ä¸åŒçš„ç²’åº¦åŒºåˆ†ï¼Œå¸¸è§çš„åˆ†è¯æ–¹æ³•æœ‰ï¼š
- word base:ä»¥è¯ä¸ºå•ä½ï¼Œä¾‹å¦‚ï¼šToday is sunday æŒ‰ç…§è¿™ç§åˆ†è¯æ–¹æ³•ä¼šè¢«åˆ†ä¸ºï¼š[Today ,is,sunday]
- character base:ä»¥å­—ç¬¦ä¸ºå•ä½ï¼Œä¾‹å¦‚ï¼š Today is sunday æŒ‰ç…§è¿™ç§åˆ†è¯æ–¹æ³•ä¼šè¢«åˆ†ä¸ºï¼š[T,o,d,a,y,i,s,u,n,d,a,y,.]
- subword base:æŒ‰ç…§è¯çš„subwordè¿›è¡Œåˆ†è¯ã€‚å¦‚è‹±æ–‡Today is sunday. åˆ™ä¼šåˆ†å‰²æˆ[To,day,is,s,un,day,.]

è‡ªGPT2å¼€å§‹ï¼Œå¤§æ¨¡å‹ä¸­çš„å¸¸è§åˆ†è¯æ–¹å¼ä¸ºç¬¬ä¸‰ç§ï¼Œå³ä»¥å­è¯çš„æ–¹å¼è¿›è¡Œåˆ†è¯ï¼Œè¿™é‡Œä»‹ç»å½“å‰é¢„è®­ç»ƒå¤§æ¨¡å‹ä¸­å¸¸è§çš„åˆ†è¯æ–¹æ³•ï¼šByte Pair Encodingï¼ˆBPEï¼‰ï¼Œæœ¬é¡¹ç›®ä¸­æ‰€ä½¿ç”¨çš„åˆ†è¯æ–¹æ³•ä¹Ÿæ˜¯åŸºäºBPEçš„ã€‚

#### 1.1 BPEç®—æ³•åŸç†

BPEï¼ˆByte Pair Encodingï¼‰ç®—æ³•æ˜¯ä¸€ç§æ•°æ®å‹ç¼©ç®—æ³•ï¼Œé€šè¿‡å°†å¸¸è§çš„å­—ç¬¦æˆ–å­—ç¬¦åºåˆ—åˆå¹¶æˆæ–°çš„å•å…ƒï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªè¯æ±‡è¡¨ï¼Œè¿™ä¸ªè¯æ±‡è¡¨å¯ä»¥åŒ…å«ä»å•ä¸ªå­—ç¬¦åˆ°å®Œæ•´å•è¯çš„å„ç§é•¿åº¦çš„å•å…ƒã€‚

ä»¥ä¸‹æ˜¯BPEç®—æ³•çš„åŸºæœ¬æ­¥éª¤ï¼š
1. åˆå§‹åŒ–ï¼šå°†è¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªå­—ç¬¦è§†ä¸ºä¸€ä¸ªå•ç‹¬çš„å•å…ƒã€‚å¯¹äºç»™å®šçš„æ–‡æœ¬æ•°æ®ï¼Œç»Ÿè®¡æ¯ä¸ªå•å…ƒï¼ˆå³å­—ç¬¦ï¼‰çš„å‡ºç°é¢‘ç‡ã€‚
2. ç»Ÿè®¡é¢‘ç‡ï¼šéå†æ–‡æœ¬æ•°æ®ï¼Œç»Ÿè®¡æ‰€æœ‰ç›¸é‚»å•å…ƒå¯¹ï¼ˆä¾‹å¦‚å­—ç¬¦å¯¹ï¼‰çš„å‡ºç°æ¬¡æ•°ã€‚
3. åˆå¹¶æœ€é¢‘ç¹çš„å•å…ƒå¯¹ï¼šé€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„å•å…ƒå¯¹è¿›è¡Œåˆå¹¶ï¼Œå½¢æˆä¸€ä¸ªæ–°çš„å•å…ƒã€‚æ›´æ–°è¯æ±‡è¡¨ï¼Œå°†æ–°å•å…ƒåŠ å…¥ï¼Œå¹¶åˆ é™¤åŸæ¥çš„ä¸¤ä¸ªå•å…ƒã€‚æ›´æ–°æ‰€æœ‰åŒ…å«è¿™ä¸¤ä¸ªå•å…ƒçš„ç»Ÿè®¡ä¿¡æ¯ï¼Œç”¨æ–°å•å…ƒæ›¿æ¢å®ƒä»¬ã€‚
4. è¿­ä»£ï¼šé‡å¤æ­¥éª¤2å’Œ3ï¼Œç›´åˆ°è¾¾åˆ°é¢„è®¾çš„è¯æ±‡è¡¨å¤§å°æˆ–è¿­ä»£æ¬¡æ•°ã€‚
5. ç”Ÿæˆè¯æ±‡è¡¨ï¼šåœ¨å®Œæˆæ‰€æœ‰è¿­ä»£åï¼Œå¾—åˆ°çš„è¯æ±‡è¡¨åŒ…å«äº†ä»å•ä¸ªå­—ç¬¦åˆ°è¾ƒé•¿å­è¯å•å…ƒçš„å„ç§é•¿åº¦çš„å•å…ƒã€‚
6. ç¼–ç ï¼šä½¿ç”¨ç”Ÿæˆçš„è¯æ±‡è¡¨å¯¹æ–°çš„æ–‡æœ¬æ•°æ®è¿›è¡Œç¼–ç ã€‚è¿™é€šå¸¸æ„å‘³ç€å°†æ–‡æœ¬æ‹†åˆ†æˆè¯æ±‡è¡¨ä¸­çš„å•å…ƒåºåˆ—ã€‚

**BPEç®—æ³•çš„ä¼˜ç‚¹åœ¨äº**ï¼š

1.èƒ½å¤Ÿå¤„ç†æœªçŸ¥å•è¯ï¼ˆOOVï¼ŒOut-of-Vocabulary wordsï¼‰ï¼Œå› ä¸ºå³ä½¿ä¸€ä¸ªå®Œæ•´çš„å•è¯ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œå®ƒçš„å­è¯å•å…ƒä¹Ÿå¯èƒ½åœ¨è¯æ±‡è¡¨ä¸­ã€‚2.ç”Ÿæˆçš„è¯æ±‡è¡¨å¤§å°å¯æ§ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œè°ƒæ•´ã€‚ç›¸å¯¹äºåŸºäºè§„åˆ™çš„åˆ†è¯æ–¹æ³•ï¼ŒBPEæ›´åŠ çµæ´»ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒè¯­è¨€çš„ç‰¹ç‚¹ã€‚

#### 1.2 åˆ†è¯å™¨è®­ç»ƒï¼ˆå¯é€‰ï¼‰
> 1.åˆ†è¯å™¨è®­ç»ƒéœ€è¦å¾ˆå¤§çš„å†…å­˜ã€‚
> 2.æœ¬äººè®­ç»ƒçš„åˆ†è¯å™¨æ˜¯åœ¨18Gè¯­æ–™ä¸­è¿›è¡Œè®­ç»ƒå¾—åˆ°çš„ï¼Œå¤§çº¦éœ€è¦600Gå†…å­˜ï¼ˆæ‚è„¸ï¼‰ï¼Œè¯­æ–™ä¸»è¦é‡‡æ ·äº†å¤©å·¥æ•°æ®é›†ï¼Œç™¾åº¦ç™¾ç§‘ï¼Œç»´åŸºç™¾ç§‘ä¸­è‹±æ–‡ï¼Œå› æ­¤å¯ä»¥æ”¯æŒä¸­è‹±æ–‡è¯­æ–™çš„åˆ†è¯ï¼Œæœ€åçš„è¯è¡¨è§„æ¨¡ä¸ºï¼š60930ï¼Œç»“æŸç¬¦ä¸ºï¼š[EOS],å¼€å§‹ç¬¦ä¸ºï¼š[BOS]

```bash
# å¦‚æœæ‚¨æƒ³è¦è®­ç»ƒè‡ªå·±çš„åˆ†è¯å™¨
# step1: å°†æ‚¨çš„è¯­æ–™æ•°æ®æ”¶é›†åœ¨wiki.txtæ–‡æœ¬æ–‡ä»¶ä¸­ï¼Œæ³¨æ„è¦æ”¾åœ¨ã€‚
# step2:
cd tokenizer_train
python 0_complex2simple.py #ç”¨äºç¹ä½“å­—è½¬æ¢ä¸ºç®€ä½“å­—
python 1_train_tokenizer.py #è®­ç»ƒä½ çš„åˆ†è¯å™¨,åŸºäºtransformersåº“ä¸­çš„BpeTrainer
```
æµ‹è¯•è®­ç»ƒå¥½çš„åˆ†è¯å™¨ï¼š
```python
from transformers import AutoModelForCausalLM,AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('./tokenize_me')
zh_demo = 'åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚ä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡ã€‚'
print(tokenizer.tokenize(zh_demo))
print(tokenizer.encode(zh_demo))

'''
['Ã¥ÂºÄ¬', 'Ã¥Ä«Ä¯', 'Ã¦ÄºÄ°Ã¦Ä¾Äª', 'Ã¥Ä§Ä«', 'Ã¯Â¼Ä®', 'Ã§Ä¸Ä³', 'Ã¦ÄºÂ¯', 'Ã¥Ä¾Â°Ã¤Â¸Ä¬', 'Ã©Ä¾Ä¾', 'Ã£Ä¢Ä¤', 'Ã¤Â¸Â¾', 'Ã¥Â¤Â´', 'Ã¦Ä¾Ä½', 'Ã¦ÄºÄ°Ã¦Ä¾Äª', 'Ã¯Â¼Ä®', 'Ã¤Â½Ä°Ã¥Â¤Â´', 'Ã¦Ä¢Ä¿', 'Ã¦Ä·Ä§Ã¤Â¹Â¡', 'Ã£Ä¢Ä¤']
[2693, 559, 29962, 1013, 249, 2725, 299, 9807, 12776, 256, 1391, 1116, 1432, 29962, 249, 39922, 1414, 20327, 256]
'''
en_demo = 'what can I say? Mamba out!'
print(tokenizer.tokenize(en_demo))
print(tokenizer.encode(en_demo))
'''
['wh', 'at', 'Ä can', 'Ä I', 'Ä say', '?', 'Ä M', 'amba', 'Ä out', '!']
[6662, 297, 2655, 539, 18606, 37, 437, 40618, 2159, 7]
'''

code_demo = 'import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n'
print(tokenizer.tokenize(code_demo))
print(tokenizer.encode(code_demo))
'''
['im', 'port', 'Ä num', 'py', 'Ä as', 'Ä n', 'p', '\n', 'im', 'port', 'Ä mat', 'pl', 'ot', 'l', 'ib', '.', 'py', 'pl', 'ot', 'Ä as', 'Ä pl', 't', '\n', 'im', 'port', 'Ä pand', 'as', 'Ä as', 'Ä p', 'd', '\n', 'im', 'port', 'Ä tor', 'ch', '\n']
[586, 1525, 2810, 42627, 640, 544, 86, 60929, 586, 1525, 5378, 1737, 550, 82, 1522, 20, 42627, 1737, 550, 640, 962, 90, 60929, 586, 1525, 21377, 347, 640, 350, 74, 60929, 586, 1525, 22572, 600, 60929]
'''
```

> ä¸Šé¢çš„åˆ†è¯å™¨æ˜¯åŸºäºtransformersåº“è¿›è¡Œè®­ç»ƒçš„ï¼Œå…³äºBPEç®—æ³•çš„ä»£ç å·²ç»è¢«å®˜æ–¹é›†æˆå¥½äº†ï¼Œå¦‚æœä½ æƒ³äº†è§£å¦‚ä½•ä½¿ç”¨python å®ç°BPEç®—æ³•ï¼Œè¯¦ç»†çš„æ­¥éª¤ä»£ç è§ **tokenizer_train/åˆ†è¯å™¨ï¼ˆtokenizeï¼‰.ipynb**


### 2. æ¨¡å‹: Transformer Decoder-only æ¡†æ¶
åœ¨MOEæ¡†æ¶è¯ç”Ÿä¹‹å‰ï¼Œå¤§æ¨¡å‹çš„åŸºæœ¬æ¡†æ¶å¤§éƒ¨åˆ†éƒ½æ˜¯åŸºäºå¦‚ä¸‹çš„ç»“æ„ï¼ˆä»¥Llama2ä¸ºä¾‹ï¼‰

![LLama2](./assets/LLama.png)


ä»¥æ¯ä¸ªdecoderå±‚ä¸ºä¾‹ï¼Œå…¶å†…éƒ¨åˆ†åˆ«åˆå«æœ‰è‡ªæ³¨æ„åŠ›å±‚ï¼ˆself-attentionï¼‰ä»¥åŠFFNå±‚ï¼Œå…¶ä¸­è‡ªæ³¨æ„åŠ›å±‚å¯ä»¥è®¡ç®—æ¯ä¸ªtokenä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå³ä¸åŒçš„å¥å­æ‰€è¡¨è¾¾çš„ä¸åŒçš„è¯­ä¹‰ä¿¡æ¯ï¼›FFNå±‚é€šè¿‡ä½¿ç”¨å¼•å…¥ä¸‰ä¸ªLinearå—ä»¥åŠSiLUæ¿€æ´»å‡½æ•°ï¼Œä»¥æ­¤æ¥æŒ–æ˜åºåˆ—ä¸­çš„tokenæ¶µç›–çš„æ›´å¤šçš„è¯­ä¹‰ä¿¡æ¯ã€‚

å…·ä½“çš„æ¨¡å‹ç»†èŠ‚è¯·çœ‹model/modeling_ndl.py,é‡Œé¢å¤§éƒ¨åˆ†ä»£ç å·²ç»è¿›è¡Œæ³¨é‡Šã€‚
### 3.æ¨¡å‹: Transformer Decoder-only MOE æ¡†æ¶


MOEæ¨¡å‹çš„ç›¸å…³å‚è€ƒå†…å®¹ï¼š
- [Deepseekâ€”MOE](https://arxiv.org/abs/2401.06066)
- [Hugging-face MOEä»‹ç»](https://huggingface.co/blog/zh/moe)

MOEæ¨¡å‹æœ€å¤§çš„ç‰¹ç‚¹æ˜¯å…·æœ‰**ç¨€ç–æ€§è´¨**ï¼Œå°†ä¼ ç»Ÿå¼€æºå¤§æ¨¡å‹ä¸­çš„FFNå±‚æ›¿æ¢ä¸ºç¨€ç–çš„MOEå±‚ï¼Œåœ¨æ¯ä¸ªMOEå±‚ä¸­ä¼šæœ‰ä¸€ä¸ªé—¨æ§å•å…ƒï¼ˆRouterï¼‰ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªç±»ä¼¼äºåˆ†ç±»å™¨çš„é—¨æ§å•å…ƒï¼Œä»è€Œå†³å®šå½“å‰çš„tokenåˆ°åº•è¾“å…¥åˆ°å“ªä¸ªä¸“å®¶ï¼ˆé€šå¸¸æ˜¯ç¼©å°ç‰ˆçš„FFNï¼‰ã€åœ¨tokenåˆ†é…ç»™ä¸“å®¶ä¹‹å‰ï¼Œä¼šå¯¹æ¯ä¸ªä¸“å®¶å¯¹è¯¥tokençš„â€œè´¡çŒ®å€¼â€è¿›è¡Œæ‰“åˆ†ï¼ˆä½äº0-1ä¹‹é—´ï¼‰ï¼Œæœ€åé€‰æ‹©top_kä¸ªä¸“å®¶åˆ†åˆ«å¯¹è¾“å…¥çš„Tokenè¿›è¡Œè¾“å‡ºã€‘ï¼Œæ™®é€šçš„MOEç»“æ„å¦‚ä¸‹ï¼š
![](./assets/switch-transformer-moe.png)

Deepseek-Moeå¯¹ä¼ ç»Ÿçš„MOEåšå‡ºä»¥ä¸‹æ”¹å˜ï¼š
- è®¾ç½®æ›´ç»†ç²’åº¦çš„ä¸“å®¶
- è®¾ç½®å…±äº«ä¸“å®¶ï¼ˆshared expertsï¼‰
- é™¤äº†è®¾ç½®ä¸“å®¶å‡è¡¡è¾…åŠ©æŸå¤±å‡½æ•°ä»¥å¤–ï¼Œè¿˜è®¾ç½®äº†é’ˆå¯¹å¤šæœºå¤šå¡è®­ç»ƒçš„è®¾å¤‡å‡è¡¡æŸå¤±å‡½æ•°

![](./assets/deepseek_moe.png)
### 4. ğŸ“˜ è®­ç»ƒæ­¥éª¤
#### 4.1 æ•°æ®ä¸‹è½½ï¼š
- [å¤©å·¥æ•°æ®é›†](https://modelscope.cn/datasets/modelscope/SkyPile-150B/files):ç”±äºå¤©å·¥æ•°æ®é›†è¾ƒå¤šï¼Œè¿™é‡Œæ¨èä½¿ç”¨git lfs è¿›è¡Œå®‰è£…:
```bash 
sudo apt update
sudo apt install git-lfs
git clone https://www.modelscope.cn/datasets/modelscope/SkyPile-150B.git
```
- [ç»´åŸºç™¾ç§‘ä¸­æ–‡æ•°æ®é›†](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)
- [ç»´åŸºç™¾ç§‘è‹±æ–‡æ•°æ®é›†]()
- [github_codeæ•°æ®é›†](https://huggingface.co/datasets/codeparrot/github-code-clean/tree/main/data)
- [ç™¾åº¦ç™¾ç§‘æ•°æ®é›†](https://huggingface.co/datasets/xuqinyang/BaiduBaike-5.63M/tree/main)

#### 4.2 æ•°æ®é¢„å¤„ç†ï¼š
é¢„è®­ç»ƒbaseæ¨¡å‹æœ€ç»ˆçš„ç›®çš„æ˜¯è®©æ¨¡å‹å…·æœ‰**ç»­å†™èƒ½åŠ›**ï¼Œä½†ä¸å¯èƒ½è®©æ¨¡å‹ä¸€ç›´ä¸æ–­åœ°ç»­å†™ä¸‹å»ï¼Œå› æ­¤éœ€è¦å†æ¯ä¸€æ®µç»“æŸçš„æ–‡æœ¬ååŠ å…¥ç»“æŸç¬¦å·ï¼Œæœ¬é¡¹ç›®çš„æ–‡æœ¬ç»“æŸç¬¦å·ä¸º'[EOS]',è¿™æ ·æ¨¡å‹åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ä¼šçŸ¥é“ä»€ä¹ˆæ—¶å€™æ˜¯å¥å­çš„ç»“æŸéƒ¨åˆ†ã€‚

è¿™é‡Œæˆ‘å‚è€ƒçš„æ˜¯ï¼šhttps://github.com/jiahe7ay/MINI_LLM/tree/main ä¸­çš„æ•°æ®é¢„å¤„ç†æ–¹æ³•ï¼Œè¯¥åšä¸»çš„æ•°æ®é¢„å¤„ç†æ–¹æ³•ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š
- èŠ‚çœå†…å­˜ï¼Œå°†é¢„å¤„ç†å¥½çš„æ•°æ®ä¿å­˜ä¸ºparquetæ ¼å¼èƒ½å¤Ÿæœ‰æ•ˆçš„èŠ‚çœä½ çš„å†…å­˜ç©ºé—´
- åˆ†è¯åçš„token_idä¼šè‡ªåŠ¨ç¼“å­˜åœ¨.cacheæ–‡ä»¶å¤¹å†…ï¼Œè¿™æ ·æœ‰åˆ©äºè¿›è¡Œæ–­ç‚¹è®­ç»ƒæˆ–è€…é‡æ–°è®­ç»ƒï¼Œä¸ä¼šå› ä¸ºæ„å¤–å‘ç”ŸOOMæˆ–è€…ç¨‹åºå‡ºé”™è¦é‡æ–°è¿›è¡Œåˆ†è¯ï¼Œç»™ä½œè€…ç‚¹èµã€‚

```bash
# å¿«é€Ÿå¼€å§‹ï¼š
# é¦–å…ˆè¦å…ˆä¸‹è½½æ‚¨éœ€è¦çš„æ•°æ®åˆ°ç›¸åº”çš„æ–‡ä»¶å¤¹ä¸­
cd utils
python data_preprocess.py
#gen_sky('/root/autodl-tmp/sky_new','/root/autodl-tmp/data/sky_new')
# ä»¥å¤„ç†å¤©å·¥æ•°æ®é›†ä¸ºä¾‹ï¼šä½ éœ€è¦å…ˆå°†å¤©å·¥æ•°æ®é›†ä¸‹è½½è‡³æ–‡ä»¶å¤¹ï¼šsky_newå†…ï¼Œæœ€ç»ˆé¢„å¤„ç†åçš„æ•°æ®ä¿å­˜åœ¨data/sky_newä¸‹é¢
```

#### 4.3 é¢„è®­ç»ƒ-NDLSLM_0.8B-base
è¯¥æ¨¡å‹çš„å…·ä½“ç»†èŠ‚è§Modelscopeï¼šhttps://modelscope.cn/models/Ndlcwx/NDLSLM_0.8B-base/summary

æœ¬æ¬¡é¢„è®­ç»ƒçš„ä¸»è¦ç‰¹ç‚¹æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š
- **æ”¯æŒbf16ä¸float32æ··åˆç²¾åº¦è®­ç»ƒ**ï¼Œåœ¨RMSNormä¸ROPEå¤„é‡‡ç”¨flaot32,éƒ¨åˆ†å‰å‘ä¼ æ’­ä¸åå‘ä¼ æ’­é‡‡ç”¨bf16ï¼Œä»¥æ­¤å¹³è¡¡å†…å­˜ä¸ç²¾åº¦ã€‚
  - float16:ä¸€ä½ç¬¦å·ä½ï¼ˆä»£è¡¨æ­£è´Ÿï¼‰ï¼Œäº”ä½æŒ‡æ•°ä½ï¼ˆä»£è¡¨èŒƒå›´ï¼‰ï¼Œåä½å°¾æ•°ï¼Œä»£è¡¨äº†ç²¾åº¦ã€‚å› æ­¤float16æ‰€è¡¨ç¤ºçš„æ•°æ®ç²¾åº¦è¾ƒä½
  - float32:ä¸€ä½ç¬¦å·ä½ï¼ˆä»£è¡¨æ­£è´Ÿï¼‰ï¼Œå…«ä½æŒ‡æ•°ä½ï¼ˆä»£è¡¨èŒƒå›´ï¼‰ï¼Œ23ä½å°¾æ•°ï¼Œä»£è¡¨äº†ç²¾åº¦ã€‚å› æ­¤float32æ‰€èƒ½è¡¨ç¤ºçš„æ•°æ®èŒƒå›´æ›´å¤§ï¼Œç²¾åº¦æ›´é«˜
  - bfloat16:ä¸€ä½ç¬¦å·ä½ï¼ˆä»£è¡¨æ­£è´Ÿï¼‰ï¼Œä½†æ˜¯å¯ä»¥è¡¨ç¤ºå’Œfloat32ä¸€æ ·çš„å…«ä½æŒ‡æ•°ï¼Œä¸ƒä½å°¾æ•°ã€‚å› æ­¤bfloat16å¯ä»¥è¡¨ç¤ºçš„æ•°æ®èŒƒå›´ä¸float32ç›¸åŒï¼Œä½†æ˜¯ç²¾åº¦ä½äºfloat16ä»¥åŠfloat32.

- **æ”¯æŒNTKç¼©æ”¾**

- **æ”¯æŒGQAã€MQA**

- **æ”¯æŒåœ¨RMSNormä½¿ç”¨flash-attnåŠ é€Ÿè®¡ç®—**



**å¦‚æœæ‚¨æ‰“ç®—ä»é›¶å¼€å§‹é¢„è®­ç»ƒNDLSLM_0.8B-baseæ¨¡å‹**ï¼š
>é¢„è®­ç»ƒä¹‹å‰è¯·æ ¹æ®æ‚¨çš„GPUæ•°é‡æƒ…å†µæ›´ accelerate_multi_gpu.yamlä¸­çš„num_processesæ•°é‡

- å¦‚æœæ‚¨æƒ³ä¿®æ”¹æ¨¡å‹çš„é…ç½®å¦‚ï¼šhidden_dim,decoder_layers,intermediate_dimç­‰ï¼Œè¯·ç›´æ¥åœ¨model_config.pyè¿›è¡Œä¿®æ”¹ï¼Œä¿®æ”¹åå¯ä»¥è¿è¡Œç›¸åº”çš„modelæ–‡ä»¶æŸ¥çœ‹æ‚¨ä¿®æ”¹çš„é…ç½®å¯¹åº”çš„æ¨¡å‹å‚æ•°å¤§å°ï¼š
```python
python modeling_ndl.py #ä¼šè¾“å‡ºè¯¥æ¨¡å‹çš„å‚æ•°é‡å¤§å°
```
å½“ç¡®å®šå¥½æ‚¨çš„æ¨¡å‹å…·ä½“å‚æ•°ä»¥åŠGPUæ•°é‡è®¾ç½®ï¼Œæ‰§è¡Œä¸‹é¢çš„bashå‘½ä»¤è¿›è¡Œé¢„è®­ç»ƒï¼š
```bash
bash run_pretrain.sh
```
- å¦‚æœæ‚¨æƒ³åœ¨æˆ‘çš„æ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œå¢é‡é¢„è®­ç»ƒï¼š
```python
# é¦–å…ˆè¦å°†NDLSLM_0.8Bæ¨¡å‹ä¸‹è½½è‡³ç›®å½•ï¼š/autodl-tmp/model_save/0_8B_base,ä¸‹è½½æ–¹æ³•ï¼š
from modelscope import snapshot_download
import os
path = '/root/autodl-tmp/model_save/0_8B_base'
if os.path.exists(path):
    pass
else:
    os.mkdir(path)
model_dir = snapshot_download('Ndlcwx/NDLSLM_0.8B-base',cache_dir = path)
```
æ¨¡å‹ä¸‹è½½ä¹‹åï¼Œæ·»åŠ æ‚¨è‡ªå·±çš„æ•°æ®é›†ï¼Œæ‰§è¡Œä¸‹é¢çš„è„šæœ¬å³å¯è¿›è¡Œå¢é‡é¢„è®­ç»ƒï¼š
<font color=Red>æ³¨æ„</font>ï¼šå¢é‡é¢„è®­ç»ƒçš„æ¨¡å‹å‚æ•°éœ€è¦å’ŒåŸæ¨¡å‹ç›¸åŒã€‚
```bash
# å»ºè®®æ‚¨é€‰æ‹©çš„æ•°æ®ä¸ºï¼šå¤©å·¥æ•°æ®é›†
bash run_ex_pretrain.sh
```

#### 4.4 NDLSLM_0.8B-base é¢„è®­ç»ƒæŸå¤±ä¸‹é™æƒ…å†µ

![](./assets/pretrain1.png)

#### 4.5 NDLSLM_0.8B-base ç»­å†™èƒ½åŠ›æµ‹è¯•
```python

from modelscope import AutoModelForCausalLM, AutoTokenizer
from modelscope import GenerationConfig
tokenizer = AutoTokenizer.from_pretrained("Ndlcwx/NDLSLM_0.8B-base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("Ndlcwx/NDLSLM_0.8B-base", device_map="auto", trust_remote_code=True).eval()
from transformers import GenerationConfig
gen_config = GenerationConfig(
    temperature=0.9,
    top_k=30,
    top_p=0.5,
    do_sample=True,
    num_beams=1,
    repetition_penalty=1.3,
    max_new_tokens=400,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)
prompt= 'é»‘é¾™æ±Ÿ ã€å‰æ—'
import torch
device = 'cuda'
tokend = tokenizer(prompt)
input_ids, attention_mask = torch.LongTensor([tokend.input_ids]).to(
        device
    ), torch.LongTensor([tokend.attention_mask]).to(device)
outputs = model.generate(
        inputs=input_ids, attention_mask=attention_mask, generation_config=gen_config
    )
outs = tokenizer.decode(outputs[0].cpu().numpy())
# outs = outs.replace(prompt,'')
outs = outs.replace('[EOS]','')
print("å½“å‰SLMå›ç­”:",outs)
'''
é»‘é¾™æ±Ÿ ã€å‰æ—ä¸œéƒ¨ã€æ±Ÿè‹åŒ—éƒ¨ç­‰åœ°éƒ¨åˆ†åœ°åŒºé™å¤§é›¨æˆ–æš´é›¨ï¼Œå››å·æˆéƒ½å’Œçœ‰å±±ã€å®‰å¾½é˜œé˜³ã€æ²³å—æ–°ä¹¡ã€å†…è’™å¤å‘¼ä¼¦è´å°”ç­‰å±€åœ°å¤§æš´é›¨ï¼ˆ100ï½170æ¯«ç±³ï¼‰ï¼Œç›åŸå±€åœ°236æ¯«ç±³ï¼ˆæœ€å¤§å°æ—¶é™é›¨é‡108æ¯«ç±³ï¼‰ã€‚ äºŒã€é‡ç‚¹å¤©æ°”é¢„æŠ¥ 1.æ±Ÿæ·®æ±Ÿæ±‰ç­‰åœ°æœ‰å¼ºé™é›¨ 8æœˆ27æ—¥è‡³28æ—¥ï¼Œæ±Ÿæ·®ã€æ±Ÿå—ä¸œåŒ—éƒ¨ã€è¥¿å—åœ°åŒºä¸œå—éƒ¨ã€é‡åº†å—éƒ¨ã€æ¹–åŒ—ä¸­éƒ¨ã€è‹çš–ä¸­åŒ—éƒ¨ã€å†…è’™å¤ä¸œåŒ—éƒ¨ã€é»‘é¾™æ±Ÿä¸œéƒ¨å’Œè¥¿åŒ—éƒ¨ç­‰åœ°çš„éƒ¨åˆ†åœ°åŒºæœ‰å¤§åˆ°æš´é›¨ï¼ˆ50ï½90æ¯«ç±³ï¼‰ã€‚ä¸Šè¿°åœ°åŒºå±€åœ°å¹¶ä¼´æœ‰çŸ­æ—¶å¼ºé™æ°´ã€é›·æš´å¤§é£ç­‰å¼ºå¯¹æµå¤©æ°”ï¼Œæœ€å¤§å°æ—¶é™é›¨é‡20ï½40æ¯«ç±³ã€‚ä¸ºæ­¤ï¼Œä¸­å¤®æ°”è±¡å°8æœˆ26æ—¥06æ—¶ç»§ç»­å‘å¸ƒå°é£è“è‰²é¢„è­¦ï¼ˆå›¾1ï¼‰ã€‚ å›¾1 å…¨å›½å¼ºé™é›¨è½åŒºé¢„æŠ¥å›¾ï¼ˆ8æœˆ27æ—¥08æ—¶-28æ—¥08æ—¶ï¼‰ 2.ä¸œåŒ—åœ°åŒºå¤§éƒ¨æœ‰é™æ°´è¿‡ç¨‹ å—é«˜ç©ºæ§½å’Œä½æ¶¡åˆ‡å˜çš„å½±å“ï¼Œæœªæ¥ä¸‰å¤©ï¼Œæ–°ç–†å—ç–†ç›†åœ°å’Œä¸­è¥¿éƒ¨åå—ã€è¥¿åŒ—åœ°åŒºä¸­ä¸œéƒ¨ã€ç”˜è‚ƒé™‡ä¸œã€é™•è¥¿ä¸­éƒ¨ç­‰åœ°æœ‰ä¸­åˆ°å¤§é›¨ï¼Œå…¶ä¸­ï¼Œæµ™æ±Ÿè¥¿éƒ¨ã€è´µå·è¥¿åŒ—éƒ¨ã€äº‘å—è¥¿å—éƒ¨ã€æ¹–å—è¥¿åŒ—éƒ¨ã€è´µå·åŒ—éƒ¨ã€å¹¿è¥¿åŒ—éƒ¨ã€å¹¿ä¸œåŒ—éƒ¨ä»¥åŠæµ·å—å²›ç­‰åœ°çš„éƒ¨åˆ†åœ°åŒºæœ‰å¤§é›¨ï¼ˆ25ï½35æ¯«ç±³ï¼‰ã€‚é’æµ·è¥¿åŒ—éƒ¨ã€è¥¿è—è¥¿åŒç‰ˆçº³ä¸­å—éƒ¨ã€è¾½å®ä¸­éƒ¨ç­‰åœ°æœ‰4ï½5çº§é£ï¼ˆè§å›¾2ï¼‰ã€‚ å›¾2 å…¨å›½é™æ°´é‡é¢„æŠ¥å›¾ï¼ˆ8æœˆ27æ—¥08æ—¶-29æ—¥08æ—¶ï¼‰ 3.ä¸»è¦æ°”è±¡è¦ç´ ä¸å¤©æ°”é¢„æµ‹æƒ…å†µ ï¼ˆä¸€ï¼‰å½“å‰å†·ç©ºæ°”æ´»åŠ¨è¾ƒä¸ºé¢‘ç¹ï¼Œå½±å“æ—¶é—´è¾ƒé•¿ï¼›
'''
```

#### 4.6 é¢„è®­ç»ƒ-NDLMoe_1.3B-base



æœ¬é¡¹ç›®ä¸­çš„NDLMoe_1.3B-baseæ¨¡å‹å€Ÿé‰´DeepSeek-Moeçš„æ¨¡å‹æ€è·¯ï¼Œå…¶ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

| Hyperparameter  |  Value |
|:----------------|:-------|
|    n_layers     |     12 |
|     n_heads     |     32 |
|     d_model     |   1600 |
| n_shared_experts|  2     |
|n_routed_experts |15|
|num_experts_per_tok| 4|
|intermediate_size|14336|
|moe_intermediate_size|1024|
|   vocab size    | 60930 |
| sequence length |   512 |
|first_k_dense_replace|1 |

æ›´ç›´è§‚çš„å‚æ•°æè¿°ï¼š
```python
NDLMOEForCausalLM(
  (model): NDLMOEModel(
    (embed_tokens): Embedding(60930, 1600)
    (norm): NDLMOEFlash_attnRMSNorm()
    (layers): ModuleList(
      (0): NDLMOEDecoderlayer(
        (self_attn): NDLSdpaAttention(
          (q): Linear(in_features=1600, out_features=1600, bias=False)
          (k): Linear(in_features=1600, out_features=1600, bias=False)
          (v): Linear(in_features=1600, out_features=1600, bias=False)
          (o): Linear(in_features=1600, out_features=1600, bias=False)
          (rotary_emb): NDLMOERotryEmbedding()
        )
        (mlp): NDLFFN(
          (ffn1_proj): Linear(in_features=1600, out_features=14336, bias=False)
          (ffn2_proj): Linear(in_features=1600, out_features=14336, bias=False)
          (o_proj): Linear(in_features=14336, out_features=1600, bias=False)
        )
        (input_layernorm): NDLMOEFlash_attnRMSNorm()
        (post_attention_layernorm): NDLMOEFlash_attnRMSNorm()
      )
      (1-11): 11 x NDLMOEDecoderlayer(
        (self_attn): NDLSdpaAttention(
          (q): Linear(in_features=1600, out_features=1600, bias=False)
          (k): Linear(in_features=1600, out_features=1600, bias=False)
          (v): Linear(in_features=1600, out_features=1600, bias=False)
          (o): Linear(in_features=1600, out_features=1600, bias=False)
          (rotary_emb): NDLMOERotryEmbedding()
        )
        (mlp): NDLMoE(
          (experts): ModuleList(
            (0-14): 15 x NDLFFN(
              (ffn1_proj): Linear(in_features=1600, out_features=1024, bias=False)
              (ffn2_proj): Linear(in_features=1600, out_features=1024, bias=False)
              (o_proj): Linear(in_features=1024, out_features=1600, bias=False)
            )
          )
          (gate): NDLMoEGate()
          (shared_experts): NDLFFN(
            (ffn1_proj): Linear(in_features=1600, out_features=2048, bias=False)
            (ffn2_proj): Linear(in_features=1600, out_features=2048, bias=False)
            (o_proj): Linear(in_features=2048, out_features=1600, bias=False)
          )
        )
        (input_layernorm): NDLMOEFlash_attnRMSNorm()
        (post_attention_layernorm): NDLMOEFlash_attnRMSNorm()
      )
    )
  )
  (lm_head): Linear(in_features=1600, out_features=60930, bias=False)
)
```

> è¯¥æ¨¡å‹æ€»å‚æ•°é‡ä¸º1.3Bï¼Œå…±æœ‰12å±‚decoderï¼Œé™¤ç¬¬ä¸€å±‚ä»¥å¤–å‡æ›¿æ¢ä¸ºMOEå±‚ï¼Œæ¯ä¸ªMOEå±‚å…±è®¾ç½®15ä¸ªå¾…æ¿€æ´»çš„ä¸“å®¶ï¼Œ2ä¸ªå…±äº«ä¸“å®¶ï¼Œæ¯æ¬¡æ¿€æ´»4ä¸ªä¸“å®¶ã€‚
> æ¿€æ´»å‚æ•°é‡çº¦ä¸ºï¼š0.5Bï¼ˆNon embeddingï¼‰ 

è¯¥æ¨¡å‹çš„å…·ä½“ç»†èŠ‚ï¼šhttps://modelscope.cn/models/Ndlcwx/NDLMoe_1.3B-base/summary

- å¦‚æœæ‚¨æ‰“ç®—ä»é›¶å¼€å§‹é¢„è®­ç»ƒNDLMoe_1.3B-baseæ¨¡å‹ï¼š
>é¢„è®­ç»ƒä¹‹å‰è¯·æ ¹æ®æ‚¨çš„GPUæ•°é‡æƒ…å†µæ›´ accelerate_multi_gpu.yamlä¸­çš„num_processesæ•°é‡

- å¼€å¯MOEæ¨¡å‹é¢„è®­ç»ƒï¼š
```bash
step1:åœ¨moe_pretrain.pyä¸­æ·»åŠ æ‚¨éœ€è¦é¢„è®­ç»ƒçš„æ•°æ®é›†
step2:è®¾ç½® run_moe_pretrain.shä¸­çš„è®­ç»ƒå‚æ•°ï¼šbatch_size,accumulation_stepsç­‰
step3:linuxç»ˆç«¯è¿è¡Œï¼š
bash run_moe_pretrain.sh
```

#### 4.7 NDLMoe_1.3B-base é¢„è®­ç»ƒæŸå¤±ä¸‹é™æƒ…å†µ
![](./assets/pretrain_moe.png)

#### 4.8 NDLMoe_1.3B-base ç»­å†™èƒ½åŠ›æµ‹è¯•
```python
from modelscope import AutoModelForCausalLM, AutoTokenizer
from modelscope import GenerationConfig
tokenizer = AutoTokenizer.from_pretrained("Ndlcwx/NDLMoe_1.3B-base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("Ndlcwx/NDLMoe_1.3B-base", device_map="auto", trust_remote_code=True).eval()
from transformers import GenerationConfig
gen_config = GenerationConfig(
    temperature=0.9,
    top_k=30,
    top_p=0.5,
    do_sample=True,
    num_beams=1,
    repetition_penalty=1.3,
    max_new_tokens=400,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)
prompt= 'é»‘é¾™æ±Ÿ ã€å‰æ—'
import torch
device = 'cuda'
tokend = tokenizer(prompt)
input_ids, attention_mask = torch.LongTensor([tokend.input_ids]).to(
        device
    ), torch.LongTensor([tokend.attention_mask]).to(device)
outputs = model.generate(
        inputs=input_ids, attention_mask=attention_mask, generation_config=gen_config
    )
outs = tokenizer.decode(outputs[0].cpu().numpy())
# outs = outs.replace(prompt,'')
# outs = outs.replace('[EOS]','')
print("å½“å‰SLMå›ç­”:",outs)
'''
é»‘é¾™æ±Ÿ ã€å‰æ—ã€è¾½å®ç­‰çœï¼ˆè‡ªæ²»åŒºï¼‰çš„31ä¸ªçœ(è‡ªæ²»åŒº)å’Œæ–°ç–†ç”Ÿäº§å»ºè®¾å…µå›¢æŠ¥å‘Šæ–°å¢ç¡®è¯Šç—…ä¾‹17ä¾‹ï¼Œå…¶ä¸­å¢ƒå¤–è¾“å…¥ç—…ä¾‹6ä¾‹ï¼›æ— æ–°å¢æ­»äº¡ç—…ä¾‹ã€‚
å½“æ—¥æ–°å¢æ²»æ„ˆå‡ºé™¢ç¡®è¯Šç—…ä¾‹1051ä¾‹ï¼Œè§£é™¤åŒ»å­¦è§‚å¯Ÿçš„å¯†åˆ‡æ¥è§¦è€…947äººï¼Œé‡ç—‡æ‚£è€…2481äººæ¬¡;å°šåœ¨é›†ä¸­éš”ç¦»åŒ»å­¦è§‚å¯Ÿæ— ç—‡çŠ¶æ„ŸæŸ“è€…518ä¾‹ã€‚[EOS]
'''
```


### 5 ğŸ“‰ ç›‘ç£å¾®è°ƒ(SFT)

SFTç›‘ç£å¾®è°ƒæ•°æ®é›†ï¼š
-[Bellä¸­æ–‡æ•°æ®é›†](https://huggingface.co/datasets/BelleGroup/train_3.5M_CN) 
-[alpaca_gpt4_data_zh](https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh/tree/main)

æœ¬é¡¹ç›®SFTç‰¹ç‚¹ï¼š
- æ”¯æŒæ–­ç‚¹è®­ç»ƒ
- æ”¯æŒæ•°æ®é¢„å¤„ç†**ç¼“å­˜æœºåˆ¶**ï¼Œç”±äºä¸Šè¿°SFTè¯­æ–™æ•´åˆåœ¨ä¸€èµ·å¤§çº¦ä¸º3.8Gå¤§å°ï¼Œå¦‚æœä¸ä½¿ç”¨ç¼“å­˜æœºåˆ¶ï¼Œæ¯æ¬¡é‡æ–°è®­ç»ƒæˆ–è€…æ–­ç‚¹è®­ç»ƒä¼šæµªè´¹ä¸å¿…è¦çš„æ—¶é—´ã€‚

#### 5.1 NDLSLM_0.8B-Chatç›‘ç£å¾®è°ƒè®­ç»ƒ

**SFTæ•°æ®é¢„å¤„ç†**ï¼š
å°†ä¸Šè¿°SFTæ•°æ®ä¸‹è½½åˆ°æœ¬åœ°ä¹‹åï¼Œæ‰§è¡ŒSFTæ•°æ®é¢„å¤„ç†ç¨‹åºï¼Œè¯¥ç¨‹åºæœ€ç»ˆä¼šç”Ÿæˆä¸€ä¸ªzh1.json,ä¸finetune.pyéœ€è¦çš„æ•°æ®æ ¼å¼ç›¸å¯¹åº”ï¼Œæ‰§è¡Œè„šæœ¬å¦‚ä¸‹ï¼š
```bash  
cd utils
python sft_data.py
```
**è¿è¡ŒSFT**ï¼š

```bash
step1:æ ¹æ®ä½ çš„èµ„æºæƒ…å†µä¿®æ”¹accelerate_multi_gpu.yamlä¸­ç›¸åº”çš„å‚æ•°
step2:é€‰æ‹©å¾®è°ƒçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™é‡Œå‡å¦‚æ‚¨ä½¿ç”¨NDLSLM_0.8B-baseæ¨¡å‹ï¼Œè¯·åœ¨run_sft.shä¸­å°†MODEL_PATH_NAMEçš„å‚æ•°è®¾ç½®æ›´æ”¹ä¸º'Ndlcwx/NDLSLM_0.8B-base'
# è¯¦ç»†çš„è®­ç»ƒç»†èŠ‚åœ¨finetune.pyä¸­
step3:linuxç»ˆç«¯æ‰§è¡Œè„šæœ¬ï¼š
bash run_sft.sh
```
NDLSLM_0.8B-Chatçš„ç›‘ç£è®­ç»ƒæƒ…å†µå¦‚ä¸‹ï¼š
![](./assets/sft_0.8Bv1.png)

**ç®€å•æµ‹è¯•NDLSLM_0.8B-Chatçš„é—®ç­”èƒ½åŠ›**ï¼š
```bash
è¿è¡Œcli_demo.pyå¯ä»¥è¿ç»­çš„å‘NDLSLM_0.8B-Chatæå‡ºé—®é¢˜
è¾“å…¥ exit é€€å‡º
è¾“å…¥ cls æ¸…é™¤å±å¹•å†…å®¹
python cli_demo.py
```
ğŸ˜·<font color=Red> æ³¨æ„ï¼šç”±äºé¢„è®­ç»ƒè¯­æ–™ä»¥åŠæ¨¡å‹å‚æ•°éƒ½æ¯”è¾ƒå°ï¼ŒChatæ¨¡å‹çš„å›ç­”ä¸ä¸€å®šæ¯æ¬¡å›ç­”çš„å¾ˆå‡†ç¡®ï¼Œæœ¬é¡¹ç›®ä¸»è¦æ˜¯å°†SFTçš„è·¯å­èµ°é€šï¼Œç¬¬ä¸€è¿½æ±‚çš„ç›®æ ‡æ˜¯è¯´äººè¯ï¼Œæ¨¡å‹çš„è¾“å‡ºä¸ä»£è¡¨æœ¬äººçš„è§‚ç‚¹å“¦</font>

![](./assets/SLM-chat1.png)
![](./assets/SLM-chat2.png)
![](./assets/SLM-chat3.png)
![](./assets/SLM-chat4.png)
![](./assets/SLM-chat5.png)
![](./assets/SLM-chat6.png)

æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨åŸºäºGurobiæ­å»ºçš„ç®€æ˜“ç½‘é¡µç‰ˆè¿›è¡Œæµ‹è¯•ï¼š
```bash
step1:
pip install -r web_demo_requirements.txt
python web_demo.py
```


#### 5.2 ç›‘ç£å¾®è°ƒ(SFT)-NDLMoe_1.3B-Chat
NDLMoe_1.3B-Chatçš„è®­ç»ƒè¯­æ–™åŒä¸Šï¼ŒæŸå¤±å‡½æ•°ä¸‹é™æƒ…å†µå¦‚ä¸‹å›¾ï¼š

![](./assets/sft_moe_1.3Bv1.png)

åŒæ ·çš„ï¼Œä½ å¯ä»¥ä½¿ç”¨cli_demo.pyå¯¹æ­¤æ¨¡å‹è¿›è¡Œæé—®ï¼Œä»¥ä¸‹æ˜¯æœ¬äººçš„å‡ ä¸ªæµ‹è¯•ç»“æœï¼š

![](./assets/Moe-chat1.png)
![](./assets/Moe-chat2.png)
![](./assets/Moe-chat3.png)
![](./assets/Moe-chat4.png)
![](./assets/Moe-chat5.png)

### 6 å¯¹Qwen_1.8B-baseè¿›è¡ŒSFT
ä¸ºäº†éªŒè¯æœ¬é¡¹ç›®SFTæµç¨‹ï¼Œæœ¬äººé€‰æ‹©äº†é€šä¹‰åƒé—®1.8Bçš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆQwen_1.8B-baseï¼‰è¿›è¡Œå…¨å‚æ•°å¾®è°ƒï¼Œä½¿ç”¨æ•°æ®ä¸å‰é¢å…¨å‚æ•°å¾®è°ƒçš„æ•°æ®ä¿æŒä¸€è‡´ã€‚
```bash
# æ³¨æ„ å¾®è°ƒqwençš„baseæ¨¡å‹æ‰€éœ€è¦çš„finetuneæ–‡ä»¶è¦ç¨å¾®æ”¹åŠ¨ä¸€ä¸‹ï¼Œä¸»è¦åœ¨åˆ†è¯å™¨ä¸Šé¢æ”¹åŠ¨ï¼šstart_idä»¥åŠend_id,å…¶ä»–åŸºæœ¬ä¿æŒä¸€è‡´
bash run_qwen_sft.sh
```
**Qwen-1.8B-SFTæŸå¤±ä¸‹é™æƒ…å†µï¼š**
![](./assets/qwen_1.8_sft.png)

**æµ‹è¯•Qwen-1.8B-SFTé—®ç­”æ•ˆæœï¼š**
ç›®å‰æœ¬äººå¾®è°ƒçš„qwen-1.8B-SFTæ¨¡å‹å·²ä¸Šä¼ è‡³[modelscope](https://modelscope.cn/models/Ndlcwx/qwen_1.8B-SFT/summary)
æ‚¨å¯ä»¥é€šè¿‡ç›´æ¥è¿è¡Œä¸‹é¢ä»£ç è¿›è¡Œä½¿ç”¨ï¼š

```python 
from modelscope import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("Ndlcwx/qwen_1.8B-SFT", revision='master', trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained("Ndlcwx/qwen_1.8B-SFT", revision='master', device_map="auto", trust_remote_code=True).eval()

response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)

response, history = model.chat(tokenizer, "ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚", history=history)
print(response)

response, history = model.chat(tokenizer, "ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜", history=history)
print(response)

response, history = model.chat(tokenizer, "è¯·å†™ä¸€æ®µPythonä»£ç ", history=history)
print(response)
```

```bash
è¿è¡Œcli_demo.pyå¯ä»¥è¿ç»­çš„å‘qwen_1.8B-SFTæå‡ºé—®é¢˜
è¾“å…¥ exit é€€å‡º
è¾“å…¥ cls æ¸…é™¤å±å¹•å†…å®¹
python cli_qwen_demo.py
```
è¿™é‡Œä»…å±•ç¤ºéƒ¨åˆ†é—®é¢˜çš„å›ç­”æ•ˆæœï¼š
![](./assets/qwen_1.8_sft_chat1.png)
![](./assets/qwen_1.8_sft_chat2.png)
![](./assets/qwen_1.8_sft_chat3.png)
![](./assets/qwen_1.8_sft_chat4.png)
![](./assets/qwen_1.8_sft_chat5.png)
![](./assets/qwen_1.8_sft_chat6.png)

**å¯ä»¥çœ‹å‡ºQwen-1.8B-SFTçš„æ•ˆæœè¿˜æ˜¯ä¸é”™çš„ï¼Œæ¯•ç«Ÿé¢„è®­ç»ƒæ¨¡å‹å¾—åˆ°äº†å……åˆ†çš„è®­ç»ƒï¼Œæ‹¥æœ‰ä¸€ä¸ªè¶³å¤Ÿâ€œèªæ˜â€çš„å¤§è„‘**
